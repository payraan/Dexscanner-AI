import asyncio
import logging
from typing import List, Dict

# --- NEW: Import the EventEngine and the Token model ---
from app.database.models import Token, Blacklist
# --- END NEW ---

from app.services.cooldown_service import token_state_service
from app.scanner.data_provider import data_provider
from app.scanner.analysis import analysis_engine
from app.core.config import settings
from app.scanner.telegram_sender import telegram_sender
from app.services.token_service import token_service
from app.scanner.token_health import token_health_checker
from app.services.bitquery_service import bitquery_service
from app.database.session import get_db
from sqlalchemy import select

from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

# Scanner configuration
BATCH_SIZE = 5  # ÿ™ÿπÿØÿßÿØ Ÿæ€åÿßŸÖ ÿ®ÿ±ÿß€å ÿßÿ±ÿ≥ÿßŸÑ ÿØÿ± Ÿáÿ± ÿØÿ≥ÿ™Ÿá
RATE_LIMIT_DELAY = 1  # ÿ™ÿßÿÆ€åÿ± 1 ÿ´ÿßŸÜ€åŸá ÿ®€åŸÜ Ÿáÿ± ÿßÿ±ÿ≥ÿßŸÑ
RANGING_THRESHOLD = 5.0  # ÿØÿ±ÿµÿØ ÿ™ÿ∫€å€åÿ± ÿ®ÿ±ÿß€å Ÿàÿ±ŸàÿØ ÿ®Ÿá ÿ≠ÿßŸÑÿ™ ranging
BREAKOUT_THRESHOLD = 7.0  # ÿØÿ±ÿµÿØ ÿ™ÿ∫€å€åÿ± ÿ®ÿ±ÿß€å ÿÆÿ±Ÿàÿ¨ ÿßÿ≤ ranging
RANGING_TIMEOUT = timedelta(hours=2)  # ÿ≠ÿØÿß⁄©ÿ´ÿ± ÿ≤ŸÖÿßŸÜ ÿØÿ± ÿ≠ÿßŸÑÿ™ ranging
MIN_UPDATE_INTERVAL = timedelta(minutes=30)  # ÿ≠ÿØÿßŸÇŸÑ ŸÅÿßÿµŸÑŸá ÿ®€åŸÜ ÿ¢ŸæÿØ€åÿ™‚ÄåŸáÿß

class TokenScanner:
   def __init__(self):
       self.running = False
       self.scan_count = 0

   async def _monitor_and_process_events(self, tokens_from_api: List[Dict]):
       """
       Monitors tokens and sends updates for all healthy tokens.
       """
       async for session in get_db():
           updates_to_send = []

           # Reset cooldown tokens at the beginning of each monitoring cycle
           await token_state_service.reset_cooled_down_tokens(session)
           
           for token_data in tokens_from_api:
               # Check if token is blacklisted
               blacklist_check = await session.execute(
                   select(Blacklist).where(Blacklist.token_address == token_data['address'])
               )
               if blacklist_check.scalar_one_or_none():
                   logger.info(f"‚õî Skipping blacklisted token: {token_data.get('symbol', 'Unknown')}")
                   continue

               # Get token record
               token_record_result = await session.execute(
                   select(Token).where(Token.address == token_data['address'])
               )
               token = token_record_result.scalar_one_or_none()
               if not token:
                   logger.warning(f"Token {token_data['symbol']} not found in DB, skipping.")
                   continue

               current_price = token_data.get('price_usd', 0)
               should_send_update = False
               
               # Skip tokens in cooldown
               if token.state in ['SIGNALED', 'COOLDOWN']:
                   continue

               # First scan logic
               if not token.last_scan_price:
                   should_send_update = True
                   token.state = 'WATCHING'
                   logger.info(f"üÜï First scan for {token.symbol}")
               else:
                   # Calculate price change
                   last_price = token.last_scan_price
                   price_change_percent = ((current_price - last_price) / last_price) * 100
                   time_since_last_update = datetime.utcnow() - token.last_state_change
                   
                   # Ranging logic with time component
                   if token.state == 'RANGING':
                       if abs(price_change_percent) > BREAKOUT_THRESHOLD or time_since_last_update > RANGING_TIMEOUT:
                           token.state = 'TRENDING'
                           should_send_update = True
                           logger.info(f"üìà {token.symbol} broke out of range!")
                   elif abs(price_change_percent) < RANGING_THRESHOLD:
                       if token.state != 'RANGING':
                           token.state = 'RANGING'
                           logger.info(f"üò¥ {token.symbol} entered ranging state")
                           token.last_state_change = datetime.utcnow()
                   
                   else:  # WATCHING or TRENDING state
                       if time_since_last_update > MIN_UPDATE_INTERVAL:
                           should_send_update = True
                           if token.state != 'TRENDING':
                               token.state = 'TRENDING'
                               token.last_state_change = datetime.utcnow()

               if should_send_update:
                   # Get analysis data
                   analysis_data, df = await analysis_engine.analyze_token(token_data)
                   if analysis_data and df is not None:
                       updates_to_send.append((analysis_data, df, token))
                       token.last_scan_price = current_price
                       token.last_state_change = datetime.utcnow()
                       logger.info(f"üì§ Queued update for {token.symbol}")

           # Batch sending with rate limiting
           if updates_to_send:
               logger.info(f"üì® Sending {len(updates_to_send)} updates in batches...")
               for update_args in updates_to_send:
                   await telegram_sender.send_signal(*update_args)
    
                   # ŸÅÿπÿßŸÑ‚Äåÿ≥ÿßÿ≤€å ÿ≥€åÿ≥ÿ™ŸÖ Cooldown
                   analysis_data, _, token = update_args
                   await token_state_service.record_signal_sent(
                       analysis_data['address'], 
                       analysis_data['price']
                   )
    
                   await asyncio.sleep(RATE_LIMIT_DELAY)
               
           await session.commit()

   async def start_scanning(self):
       """Main scanning loop - now simplified to an event monitoring loop."""
       self.running = True
       logger.info(f"Event-Driven Scanner started. Scan interval: {settings.SCAN_INTERVAL} seconds")

       while self.running:
           try:
               self.scan_count += 1
               logger.info(f"--- Starting Monitoring Cycle #{self.scan_count} ---")
               
               tokens = await data_provider.fetch_trending_tokens(limit=settings.TRENDING_TOKENS_LIMIT)

               if tokens:
                   # This step remains to keep our token list up-to-date
                   await token_service.store_tokens_with_health(tokens)
                   logger.info(f"Fetched and stored {len(tokens)} tokens.")
                   
                   # The main logic is now here
                   await self._monitor_and_process_events(tokens)
               else:
                   logger.warning("No trending tokens found in this monitoring cycle.")
               
               logger.info(f"--- Monitoring Cycle #{self.scan_count} Completed ---")
               await asyncio.sleep(settings.SCAN_INTERVAL)

           except Exception as e:
               logger.error(f"CRITICAL ERROR in monitoring loop #{self.scan_count}: {e}", exc_info=True)
               await asyncio.sleep(60)

   def stop(self):
       self.running = False
       logger.info("Scanner stopped")

token_scanner = TokenScanner()
